{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "from bs4 import BeautifulSoup\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function to pull a maximum of ~2,500 reddit posts and all associated comments\n",
    "def reddit_pull(url, max_pull_size):\n",
    "    \n",
    "    ## Setting global variables so they can be used in later functions\n",
    "    global headers\n",
    "    global errors\n",
    "    global comments_list\n",
    "    \n",
    "    ## Establishing login creds and empty lists\n",
    "    errors = []\n",
    "    posts_list = []\n",
    "    comments_list = []\n",
    "    \n",
    "    ## Setting after to random string so that loop does not break\n",
    "    after = 'randomstring'\n",
    "    \n",
    "    ## Continuing to loop until there are as many posts as the defined/passed max pull size\n",
    "    while len(posts_list) < max_pull_size:\n",
    "        print('Pulling post data...\\n') ## Sign posting progress\n",
    "        if after == None: ## Breaks loop when last post is reached\n",
    "            break\n",
    "            \n",
    "        ## After indicates to reddit API where I want to start pulling after\n",
    "        res = requests.get(url, params = {'after' : after}, headers={'User-agent' : 'jimtronic'})\n",
    "        json_pull = res.json() ## Storing pull as JSON file\n",
    "        print('Done pulling post data.\\n') ## Sign posting progress\n",
    "        posts_list.extend(json_pull['data']['children']) ## Adding ALL of the JSON pull to my dataset\n",
    "        after = json_pull['data']['after'] ## Setting after to indicate where to start next json pull\n",
    "        time.sleep(1) ## Pausing so API doesn't throttle me\n",
    "        \n",
    "        append_comms(json_pull) ## Calling function to pull and append data from each comment page\n",
    "        \n",
    "        ## Sign posting progress\n",
    "        clear_output()\n",
    "        print(f\"Total Posts Pulled: {len(posts_list)}\\nTotal Comments Pulled: {len(comments_list)}\" +\\\n",
    "    f\"\\nComment Page Errors: {errors}\\n\")\n",
    "    \n",
    "    ## Converts complex list of dicts into simple list of dict for easy CSV export\n",
    "    clean_posts_list = data_structure_posts(posts_list)\n",
    "    \n",
    "    ## Returning list of dicts for both posts and comments\n",
    "    return clean_posts_list, comments_list ###### EDIT EDIT EDIT\n",
    "\n",
    "###### Would like to add function to re-run errors ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function to pull comments for each reddit post\n",
    "def append_comms(json_pull):\n",
    "    print('Pulling comments data...')\n",
    "    \n",
    "    ## Create a list of all the posts from the recent/passed JSON pull\n",
    "    json_list = json_pull['data']['children']\n",
    "    for i, json_item in enumerate(json_list):\n",
    "        print(f'Pulling comments for post... {i+1} of {len(json_list)}')\n",
    "        \n",
    "        ## Storing the post's comment page url as \"com_url\"\n",
    "        com_url = 'https://www.reddit.com/' + json_item['data']['permalink'][:-1] + \".html\"\n",
    "        com_res = requests.get(com_url, headers={'User-agent' : 'jimtronic'}) ## Getting data on comment page\n",
    "        com_soup = BeautifulSoup(com_res.content, 'html') ## Bringing data into Beautiful Soup\n",
    "        \n",
    "        ## Capturing the entire comments section\n",
    "        comment_section = com_soup.find('div', {'class':'p0SYO8TbZVqJIWEeFcNZx i6gx00-2 hNwzqg'})\n",
    "        \n",
    "        ## Try to find all the individual comment blocks\n",
    "        try:\n",
    "            comment_section.find_all('div', {'class' : 'Comment'})\n",
    "            \n",
    "        ## If comment blocks not found (None type), there's an error with that page\n",
    "        except:\n",
    "            print('Page error: ' + str(com_url))\n",
    "            errors.append(com_url)\n",
    "            break ### ERRORS STORED -- not sure why they're happening... throttling?\n",
    "        \n",
    "        ## Iterating through each comment on the comments page\n",
    "        for com in comment_section.find_all('div', {'class' : 'Comment'}):\n",
    "            try: ## Try adding the user name and comment text\n",
    "                user = com.find('div', {'class' : 'xvda30-0'}).text\n",
    "                text = com.find('div', {'data-test-id': 'comment'}).text\n",
    "            except: ## If it can't, it's b/c the comment was removed\n",
    "                user = 'Removed_comment'\n",
    "                text = 'Deleted comment'\n",
    "            try: ## Try to find the comment score\n",
    "                points = float(com.find('span', {'class' : 'h5svje-0 cFQOcm'}).text.strip(' points'))\n",
    "            except: ## If it can't, it's b/c the score was hidden\n",
    "                points = \"Score hidden\"\n",
    "                \n",
    "            ## Append the dictionary to the end of the comments list of dicts\n",
    "            comments_list.append({\n",
    "                'reply_to': com_url, ## So I know which post responded to\n",
    "                'user': user,\n",
    "                'text': text,\n",
    "                'points': points,\n",
    "                ## Indicates how many replies deep the reply was\n",
    "                'level': float(com.find('span', {'class': 's1dqr9jy-0 imyGpC'}).text.strip('level '))})\n",
    "\n",
    "###### Would like to add something to convert '6 hours ago' to a date-time feature... ######\n",
    "###### Would like to use the API the whole time... why am I switching to web scraping? ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Takes entire API pull of post data and returns organized structure of useful data\n",
    "def data_structure_posts(posts_list):\n",
    "    clean_posts_list = [] ## Where useful data will be stored\n",
    "    for post in posts_list: ## Loops through a list of posts\n",
    "        clean_posts_list.append({\n",
    "            'likes' : post['data']['likes'],\n",
    "            'up_votes' : post['data']['ups'],\n",
    "            'down_votes' : post['data']['downs'],\n",
    "            'title' : post['data']['title'],\n",
    "            'text' : post['data']['selftext'],\n",
    "            'author' : post['data']['author'],\n",
    "            'num_crossposts' : post['data']['num_crossposts'],\n",
    "            'is_crosspostable' : post['data']['is_crosspostable'],\n",
    "            'num_comments' : post['data']['num_comments'],\n",
    "            'score' : post['data']['score'],\n",
    "            'permalink' : 'https://www.reddit.com/' + str(post['data']['permalink']), ## To reference comments \n",
    "            'name' : post['data']['name'],\n",
    "            'url' : post['data']['url'],\n",
    "            'media' : post['data']['media'],\n",
    "            'num_reports' : post['data']['num_reports']\n",
    "        })\n",
    "    return clean_posts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function to remove duplicates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Posts Pulled: 844\n",
      "Total Comments Pulled: 5062\n",
      "Comment Page Errors: ['https://www.reddit.com//r/Conservative/comments/b7ww6s/please_post_articles_on_rconservativearticles_and.html', 'https://www.reddit.com//r/Conservative/comments/b7e5mb/university_of_california_uses_trumps_executive.html', 'https://www.reddit.com//r/Conservative/comments/b64h41/republicans_accused_of_colluding_with_reality_to.html']\n",
      "\n",
      "Pulling post data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url_conservative = 'https://www.reddit.com/r/conservative.json'\n",
    "posts_conservative, comments_conservative = reddit_pull(url_conservative, max_pull_size = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Posts Pulled: 563\n",
      "Total Comments Pulled: 4483\n",
      "Comment Page Errors: ['https://www.reddit.com//r/Liberal/comments/aalzy7/maine_gop_governor_paul_lepage_writes_stolen.html']\n",
      "\n",
      "Pulling post data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url_liberal = 'https://www.reddit.com/r/liberal.json'\n",
    "posts_liberal, comments_liberal = reddit_pull(url_liberal, max_pull_size = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lib_posts = pd.DataFrame(posts_liberal)\n",
    "df_lib_comments = pd.DataFrame(comments_liberal)\n",
    "df_cons_posts = pd.DataFrame(posts_conservative)\n",
    "df_cons_comments = pd.DataFrame(comments_conservative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lib_posts.to_csv('../data/liberal_posts.csv', index=False)\n",
    "df_lib_comments.to_csv('../data/liberal_comments.csv', index=False)\n",
    "df_cons_posts.to_csv('../data/conservative_posts.csv', index=False)\n",
    "df_cons_comments.to_csv('../data/conservative_comments.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
