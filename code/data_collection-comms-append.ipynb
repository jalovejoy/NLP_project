{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "from bs4 import BeautifulSoup\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Pulling Reddit Data\n",
    "Data for posts is pulled through the Reddit API. For each post, the permalink is used to do an additional webscrape of each post's comments page. All comments are appended as a single comment block in a column of the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function to pull a maximum of ~2,500 reddit posts and all associated comments\n",
    "def reddit_pull(url, max_pull_size):\n",
    "    \n",
    "    ## Setting global variables so they can be used in later functions\n",
    "    global errors\n",
    "    \n",
    "    ## Establishing login creds and empty lists\n",
    "    errors = []\n",
    "    posts_list = []\n",
    "    \n",
    "    ## Setting after to random string so that loop does not break\n",
    "    after = 'randomstring'\n",
    "    \n",
    "    print(f'Pulling post data for up to {max_pull_size} posts...\\n') ## Sign posting progress\n",
    "    \n",
    "    ## Continuing to loop until there are as many posts as the defined/passed max pull size\n",
    "    while len(posts_list) < max_pull_size:\n",
    "        if after == None: \n",
    "            break ## Breaks loop when last post is reached\n",
    "            \n",
    "        ## After indicates to reddit API where I want to start pulling after\n",
    "        res = requests.get(url, params = {'after' : after}, headers={'User-agent' : 'jimtronic'})\n",
    "        json_pull = res.json() ## Storing pull as JSON file\n",
    "        \n",
    "        ## Add cleaned version of each of the pulled posts to the post list\n",
    "        posts_list.extend(data_structure_posts(json_pull['data']['children']))\n",
    "        \n",
    "        after = json_pull['data']['after'] ## Setting 'after' to indicate where to start next json pull\n",
    "        time.sleep(1) ## Pausing so API doesn't throttle us\n",
    "        print(f\"Total Posts Pulled: {len(posts_list)}\")\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "    print(f'Pulling comments data for {len(posts_list)} posts...') ## User sign posting\n",
    "    for i, post in enumerate(posts_list):\n",
    "        if i % 50 == 0:\n",
    "            print(f'Pulling comments data for post {i+1}-{min(i+50,len(posts_list))} of {len(posts_list)}')\n",
    "        post = append_comms(post)\n",
    "        \n",
    "    clear_output()\n",
    "    \n",
    "    print(f\"Total Posts Pulled: {len(posts_list)}\\n\" + f\"\\nComment Page Errors: {errors}\")\n",
    "    \n",
    "    ## Returning list of dicts for both posts and comments\n",
    "    return posts_list\n",
    "\n",
    "################################\n",
    "## MAY ADD LATER\n",
    "## Function to remove duplicates...\n",
    "## Function to retry errors\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function to append comments as a single text block associated with a post\n",
    "def append_comms(post):\n",
    "\n",
    "    comments_list = []\n",
    "    num_deleted_comments = 0 ## Keeps track deleted comments\n",
    "    \n",
    "    com_url = post['permalink'][:-1] + '.html' ## Storing the post's comment page url as \"com_url\"\n",
    "    com_res = requests.get(com_url, headers={'User-agent' : 'jimtronic'}) ## Getting data on comment page\n",
    "    com_soup = BeautifulSoup(com_res.content, 'html') ## Bringing data into Beautiful Soup\n",
    "\n",
    "    ## Capturing the entire comments section\n",
    "    comment_section = com_soup.find('div', {'class':'p0SYO8TbZVqJIWEeFcNZx'})\n",
    "\n",
    "    ############################\n",
    "    #### ERROR HANDLING\n",
    "    try: ## Try to find a comment block\n",
    "        comment_section.find('div', {'class' : 'Comment'})\n",
    "\n",
    "    ## If comment blocks not found (None type), there's an error with that page\n",
    "    except:\n",
    "        print('Page error: ' + str(com_url)) ## Let user know\n",
    "        errors.append(com_url) ## Append that broken URL so user can see it\n",
    "        post['comments_text'] = '' ## Append blank comment text\n",
    "        post['num_deleted_comments'] = 0 ## 0 Comments deleted from post\n",
    "        return ''\n",
    "    ############################\n",
    "    \n",
    "    ## Iterating through each comment on the comments page\n",
    "    for com in comment_section.find_all('div', {'class' : 'Comment'}):\n",
    "        \n",
    "\n",
    "        try: ## Try adding the user name and comment text\n",
    "            ## Removing auto-mod comments\n",
    "            if com.find('div', {'class' : 'xvda30-0 camSYk'}).text != 'AutoModerator': \n",
    "                comments_list.append(com.find('div', {'data-test-id': 'comment'}).text.replace(',',''))\n",
    "        except: ## If it can't, it's b/c the comment was removed\n",
    "            text = ''\n",
    "            num_deleted_comments =+1\n",
    "\n",
    "    comment_string = ' '.join(comments_list) ## Joins each comment into one string block\n",
    "    post['comments_text'] = comment_string ## Sets comment block as value in the specific post\n",
    "    post['num_deleted_comments'] = num_deleted_comments ## Sets deleted comments in specific post\n",
    "    return post\n",
    "\n",
    "################################\n",
    "## MAY ADD LATER\n",
    "## Use the API instead of switching to web scraping?...\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Takes list of post data and returns organized structure of useful data\n",
    "def data_structure_posts(posts_list):\n",
    "    clean_posts_list = [] ## Where useful data will be stored\n",
    "    for post in posts_list: ## Loops through a list of posts\n",
    "        clean_posts_list.append({\n",
    "            'up_votes' : post['data']['ups'],\n",
    "            'down_votes' : post['data']['downs'],\n",
    "            'title' : post['data']['title'].replace(',',''),\n",
    "            'text' : post['data']['selftext'].replace(',',''),\n",
    "            'author' : post['data']['author'],\n",
    "            'num_crossposts' : post['data']['num_crossposts'],\n",
    "            'is_crosspostable' : post['data']['is_crosspostable'],\n",
    "            'num_comments' : post['data']['num_comments'],\n",
    "            'score' : post['data']['score'],\n",
    "            'permalink' : 'https://www.reddit.com' + str(post['data']['permalink']),\n",
    "            'name' : post['data']['name'],\n",
    "            'url' : post['data']['url']\n",
    "        })\n",
    "    return clean_posts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Posts Pulled: 826\n",
      "\n",
      "Comment Page Errors: ['https://www.reddit.com/r/Conservative/comments/b7k4r7/us_struggling_with_growing_number_of_asylum.html', 'https://www.reddit.com/r/Conservative/comments/b7hwtq/williams_more_university_corruption.html']\n"
     ]
    }
   ],
   "source": [
    "url_conservative = 'https://www.reddit.com/r/conservative.json'\n",
    "posts_conservative = reddit_pull(url_conservative, max_pull_size = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Posts Pulled: 568\n",
      "\n",
      "Comment Page Errors: []\n"
     ]
    }
   ],
   "source": [
    "url_liberal = 'https://www.reddit.com/r/liberal.json'\n",
    "posts_liberal = reddit_pull(url_liberal, max_pull_size = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cons_posts = pd.DataFrame(posts_conservative)\n",
    "df_lib_posts = pd.DataFrame(posts_liberal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Exporting the Data\n",
    "PKL files were the preferred export – CSV experiences data loss since some comment blocks exceed CSV's 32,767 character limit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lib_posts.to_csv('../data/liberal_posts_comms.csv', index=False)\n",
    "df_cons_posts.to_csv('../data/conservative_posts_comms.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving to pkl file because some comment blocks exceed CSV limits\n",
    "df_lib_posts.to_pickle('../data/liberal_posts_comms.pkl')\n",
    "df_cons_posts.to_pickle('../data/conservative_posts_comms.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Notes & Considerations\n",
    "- DataFrames may need to be de-duped\n",
    "- Relying solely on the API, rather than doing a webscrape, may be the preferred way of eliminating errors\n",
    "- If not, adding a function to retry errors may work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
